#!/usr/bin/env python3
"""
ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš© ì˜ˆì‹œ:
python3 scripts/load_test.py --url http://localhost:8000/api/v1/products/42 --concurrency 50 --duration 30s
python3 scripts/load_test.py --url http://localhost:8000/api/v1/products --concurrency 100 --requests 1000
"""
import argparse
import asyncio
import json
import logging
import sys
import time
from collections import defaultdict, Counter
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import statistics

import aiohttp

# ìƒìœ„ ëª¨ë“ˆ importë¥¼ ìœ„í•œ path ì„¤ì •
sys.path.append(str(Path(__file__).parent))
from db_monitor import DatabaseMonitor


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    status_code: int
    response_time: float
    error: Optional[str] = None
    timestamp: float = 0.0
    request_id: int = 0


class LoadTester:
    """ë¹„ë™ê¸° ë¶€í•˜ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, url: str, concurrency: int):
        self.url = url
        self.concurrency = concurrency
        self.results: List[TestResult] = []
        self.start_time = 0
        self.end_time = 0
        self.request_counter = 0
        self.results_dir = None  # _setup_loggerì—ì„œ ì„¤ì •ë¨
        self.logger = self._setup_logger()
        self.db_monitor = DatabaseMonitor()
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        # ê²°ê³¼ í´ë” ìƒì„±
        self.results_dir = Path('results') / datetime.now().strftime('%Y%m%d_%H%M%S')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        logger = logging.getLogger('load_test')
        logger.setLevel(logging.INFO)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        log_file = self.results_dir / 'load_test.log'
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¶”ê°€
        logger.handlers.clear()
        logger.addHandler(handler)
        
        return logger
    
    async def make_request(self, session: aiohttp.ClientSession) -> TestResult:
        """ë‹¨ì¼ HTTP ìš”ì²­ ì‹¤í–‰"""
        self.request_counter += 1
        request_id = self.request_counter
        timestamp = time.time()
        start = time.time()
        
        try:
            async with session.get(self.url) as response:
                await response.text()  # ì‘ë‹µ ë³¸ë¬¸ì„ ì½ì–´ì„œ ì™„ì „íˆ ì²˜ë¦¬
                end = time.time()
                response_time = end - start
                response_time_ms = response_time * 1000
                
                # ìš”ì²­-ì‘ë‹µ ì‹œê°„ ë¡œê¹… (ms ë‹¨ìœ„)
                self.logger.info(f"Request {request_id}: {response.status} - {response_time_ms:.2f}ms")
                
                return TestResult(
                    status_code=response.status,
                    response_time=response_time,
                    timestamp=timestamp,
                    request_id=request_id
                )
        except Exception as e:
            end = time.time()
            response_time = end - start
            response_time_ms = response_time * 1000
            
            # ì—ëŸ¬ ë¡œê¹…
            self.logger.error(f"Request {request_id}: ERROR - {response_time_ms:.2f}ms - {str(e)}")
            
            return TestResult(
                status_code=0,
                response_time=response_time,
                error=str(e),
                timestamp=timestamp,
                request_id=request_id
            )
    
    async def worker(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore, 
                    stop_event: asyncio.Event):
        """ì›Œì»¤ ì½”ë£¨í‹´ - ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´"""
        while not stop_event.is_set():
            async with semaphore:
                if stop_event.is_set():
                    break
                result = await self.make_request(session)
                self.results.append(result)
    
    async def run_duration_test(self, duration: int):
        """ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self._test_duration = duration
        semaphore = asyncio.Semaphore(self.concurrency)
        stop_event = asyncio.Event()
        
        timeout = aiohttp.ClientTimeout(total=10)
        connector = aiohttp.TCPConnector(limit=self.concurrency * 2)
        
        # DB ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.db_monitor.start_monitoring(interval=0.5)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            # ì›Œì»¤ íƒœìŠ¤í¬ë“¤ ìƒì„±
            workers = [
                asyncio.create_task(self.worker(session, semaphore, stop_event))
                for _ in range(self.concurrency)
            ]
            
            self.start_time = time.time()
            
            # ì§€ì •ëœ ì‹œê°„ ëŒ€ê¸°
            await asyncio.sleep(duration)
            
            # ëª¨ë“  ì›Œì»¤ ì¤‘ì§€
            stop_event.set()
            
            # ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
            await asyncio.gather(*workers, return_exceptions=True)
            
            self.end_time = time.time()
        
        # DB ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.db_monitor.stop_monitoring()
    
    async def run_request_count_test(self, total_requests: int):
        """ì§€ì •ëœ ìš”ì²­ ìˆ˜ë§Œí¼ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self._test_requests = total_requests
        semaphore = asyncio.Semaphore(self.concurrency)
        
        timeout = aiohttp.ClientTimeout(total=10)
        connector = aiohttp.TCPConnector(limit=self.concurrency * 2)
        
        # DB ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.db_monitor.start_monitoring(interval=0.5)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            self.start_time = time.time()
            
            # íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            for _ in range(total_requests):
                task = asyncio.create_task(self.make_request_with_semaphore(session, semaphore))
                tasks.append(task)
            
            # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            self.end_time = time.time()
            
            # ê²°ê³¼ ì €ì¥
            for result in results:
                if isinstance(result, TestResult):
                    self.results.append(result)
        
        # DB ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.db_monitor.stop_monitoring()
    
    async def make_request_with_semaphore(self, session: aiohttp.ClientSession, 
                                        semaphore: asyncio.Semaphore) -> TestResult:
        """ì„¸ë§ˆí¬ì–´ì™€ í•¨ê»˜ ìš”ì²­ ì‹¤í–‰"""
        async with semaphore:
            return await self.make_request(session)
    
    def print_results(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        if not self.results:
            print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_time = self.end_time - self.start_time
        total_requests = len(self.results)
        
        # ìƒíƒœ ì½”ë“œë³„ í†µê³„
        status_counts = Counter(result.status_code for result in self.results)
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„ (ì„±ê³µ ìš”ì²­ë§Œ)
        response_times = [result.response_time for result in self.results if result.error is None and 200 <= result.status_code < 400]
        
        # ì—ëŸ¬ í†µê³„ (4xx, 5xx ë˜ëŠ” ì˜ˆì™¸)
        errors = [result for result in self.results if result.error is not None or result.status_code >= 400]
        
        print("=" * 60)
        print("ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 60)
        print(f"URL: {self.url}")
        print(f"ë™ì‹œì„± (Concurrency): {self.concurrency}")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ì´ ìš”ì²­ ìˆ˜: {total_requests}")
        print(f"ì´ˆë‹¹ ìš”ì²­ ìˆ˜ (RPS): {total_requests / total_time:.2f}")
        
        print("\nìƒíƒœ ì½”ë“œë³„ í†µê³„:")
        for status_code, count in sorted(status_counts.items()):
            percentage = (count / total_requests) * 100
            print(f"  {status_code}: {count}íšŒ ({percentage:.1f}%)")
        
        if response_times:
            print("\nì‘ë‹µ ì‹œê°„ í†µê³„:")
            print(f"  í‰ê· : {statistics.mean(response_times):.3f}ì´ˆ")
            print(f"  ì¤‘ê°„ê°’: {statistics.median(response_times):.3f}ì´ˆ")
            print(f"  ìµœì†Œê°’: {min(response_times):.3f}ì´ˆ")
            print(f"  ìµœëŒ€ê°’: {max(response_times):.3f}ì´ˆ")
            if len(response_times) > 1:
                print(f"  í‘œì¤€í¸ì°¨: {statistics.stdev(response_times):.3f}ì´ˆ")
            
            # ë°±ë¶„ìœ„ìˆ˜
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)]
            p90 = sorted_times[int(len(sorted_times) * 0.9)]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
            
            print(f"  50th percentile: {p50:.3f}ì´ˆ")
            print(f"  90th percentile: {p90:.3f}ì´ˆ")
            print(f"  95th percentile: {p95:.3f}ì´ˆ")
            print(f"  99th percentile: {p99:.3f}ì´ˆ")
        
        if errors:
            print(f"\nì—ëŸ¬ ë°œìƒ: {len(errors)}ê±´")
            # HTTP ì—ëŸ¬ ìƒíƒœ ì½”ë“œë³„ í†µê³„
            http_errors = Counter(result.status_code for result in errors if result.status_code >= 400)
            if http_errors:
                print("  HTTP ì—ëŸ¬ ìƒíƒœ ì½”ë“œ:")
                for status_code, count in http_errors.most_common():
                    print(f"    {status_code}: {count}íšŒ")
            
            # ì˜ˆì™¸ ì—ëŸ¬ í†µê³„
            exception_errors = Counter(result.error for result in errors if result.error is not None)
            if exception_errors:
                print("  ì˜ˆì™¸ ì—ëŸ¬:")
                for error_type, count in exception_errors.most_common(3):
                    print(f"    {error_type}: {count}íšŒ")
        
        # baseline.json ì €ì¥
        self.save_baseline_results()
        
        print("=" * 60)


    def save_baseline_results(self):
        """baseline.jsonì— í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        if not self.results:
            return
        
        total_time = self.end_time - self.start_time
        total_requests = len(self.results)
        
        # ì„±ê³µí•œ ìš”ì²­ë§Œ í•„í„°ë§ (2xx, 3xx ìƒíƒœ ì½”ë“œ + ì—ëŸ¬ ì—†ìŒ)
        successful_requests = [r for r in self.results if r.error is None and 200 <= r.status_code < 400]
        response_times = [r.response_time for r in successful_requests]
        
        # ì—ëŸ¬ìœ¨ ê³„ì‚° (4xx, 5xx ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ)
        error_count = len([r for r in self.results if r.error is not None or r.status_code >= 400])
        error_rate = (error_count / total_requests) * 100 if total_requests > 0 else 0
        
        # QPS ê³„ì‚°
        qps = total_requests / total_time if total_time > 0 else 0
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        p50 = p95 = p99 = 0
        if response_times:
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)] * 1000  # ms ë³€í™˜
            p95 = sorted_times[int(len(sorted_times) * 0.95)] * 1000
            p99 = sorted_times[int(len(sorted_times) * 0.99)] * 1000
        
        baseline_data = {
            "timestamp": datetime.now().isoformat(),
            "url": self.url,
            "concurrency": self.concurrency,
            "duration_seconds": total_time,
            "total_requests": total_requests,
            "successful_requests": len(successful_requests),
            "qps": round(qps, 2),
            "error_rate_percent": round(error_rate, 2),
            "response_times_ms": {
                "p50": round(p50, 2),
                "p95": round(p95, 2),
                "p99": round(p99, 2),
                "avg": round(statistics.mean(response_times) * 1000, 2) if response_times else 0,
                "min": round(min(response_times) * 1000, 2) if response_times else 0,
                "max": round(max(response_times) * 1000, 2) if response_times else 0
            },
            "status_codes": dict(Counter(r.status_code for r in self.results)),
            "test_config": {
                "test_type": "duration" if hasattr(self, '_test_duration') else "request_count",
                "target_duration": getattr(self, '_test_duration', None),
                "target_requests": getattr(self, '_test_requests', None)
            }
        }
        
        # baseline.json ì €ì¥ (ê²°ê³¼ í´ë”ì—)
        baseline_file = self.results_dir / 'baseline.json'
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        # ìƒì„¸ ê²°ê³¼ íŒŒì¼ ì €ì¥
        detailed_results = {
            "test_summary": baseline_data,
            "raw_results": [
                {
                    "request_id": result.request_id,
                    "timestamp": result.timestamp,
                    "status_code": result.status_code,
                    "response_time_ms": round(result.response_time * 1000, 2),
                    "error": result.error,
                    "is_success": result.error is None and 200 <= result.status_code < 400
                }
                for result in self.results
            ]
        }
        
        detailed_file = self.results_dir / 'detailed_results.json'
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # DB ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì €ì¥
        db_stats_file = self.results_dir / 'db_monitoring.json'
        self.db_monitor.save_stats(db_stats_file)
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥ (DB ì •ë³´ í¬í•¨)
        summary_file = self.results_dir / 'summary.txt'
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ“Š ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½\n")
            f.write(f"=" * 40 + "\n")
            f.write(f"URL: {self.url}\n")
            f.write(f"ë™ì‹œì„±: {self.concurrency}\n")
            f.write(f"QPS: {qps:.2f}\n")
            f.write(f"P50: {p50:.2f}ms, P95: {p95:.2f}ms, P99: {p99:.2f}ms\n")
            f.write(f"ì—ëŸ¬ìœ¨: {error_rate:.2f}%\n")
            f.write(f"ì´ ìš”ì²­: {total_requests}ê±´\n")
            f.write(f"ì„±ê³µ ìš”ì²­: {len(successful_requests)}ê±´\n")
            f.write(f"\n--- DB ë¶€í•˜ í†µê³„ ---\n")
            
            # DB ìš”ì•½ ì •ë³´ ì¶”ê°€
            if self.db_monitor.stats:
                db_query_times = [s.get('db_query_time_ms') for s in self.db_monitor.stats if s.get('db_query_time_ms')]
                cpu_values = [s.get('cpu_percent', 0) for s in self.db_monitor.stats]
                memory_values = [s.get('memory_percent', 0) for s in self.db_monitor.stats]
                
                if db_query_times:
                    f.write(f"DB ì¿¼ë¦¬ ì‹œê°„: í‰ê·  {sum(db_query_times)/len(db_query_times):.2f}ms, ìµœëŒ€ {max(db_query_times):.2f}ms\n")
                if cpu_values:
                    f.write(f"CPU ì‚¬ìš©ë¥ : í‰ê·  {sum(cpu_values)/len(cpu_values):.1f}%, ìµœëŒ€ {max(cpu_values):.1f}%\n")
                if memory_values:
                    f.write(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : í‰ê·  {sum(memory_values)/len(memory_values):.1f}%, ìµœëŒ€ {max(memory_values):.1f}%\n")
        
        print(f"\nğŸ“Š ê²°ê³¼ê°€ {self.results_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   - baseline.json: ì„±ëŠ¥ ê¸°ì¤€ì„  ë°ì´í„°")
        print(f"   - detailed_results.json: ìƒì„¸ ê²°ê³¼ ë°ì´í„°")
        print(f"   - load_test.log: ìš”ì²­/ì‘ë‹µ ë¡œê·¸")
        print(f"   - db_monitoring.json: DB ë¶€í•˜ ëª¨ë‹ˆí„°ë§ ë°ì´í„°")
        print(f"   - summary.txt: ê²°ê³¼ ìš”ì•½")
        print(f"   - QPS: {qps:.2f}")
        print(f"   - P50: {p50:.2f}ms, P95: {p95:.2f}ms, P99: {p99:.2f}ms")
        print(f"   - ì—ëŸ¬ìœ¨: {error_rate:.2f}%")
        
        # DB ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì¶œë ¥
        self.db_monitor.print_summary()


def parse_duration(duration_str: str) -> int:
    """ì§€ì†ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: 30s, 2m, 1h)"""
    if duration_str.endswith('s'):
        return int(duration_str[:-1])
    elif duration_str.endswith('m'):
        return int(duration_str[:-1]) * 60
    elif duration_str.endswith('h'):
        return int(duration_str[:-1]) * 3600
    else:
        return int(duration_str)


def main():
    parser = argparse.ArgumentParser(description='HTTP ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬')
    parser.add_argument('--url', required=True, help='í…ŒìŠ¤íŠ¸í•  URL')
    parser.add_argument('--concurrency', '-c', type=int, default=10, 
                       help='ë™ì‹œ ì—°ê²° ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    
    # ì§€ì†ì‹œê°„ ë˜ëŠ” ìš”ì²­ ìˆ˜ ì¤‘ í•˜ë‚˜ ì„ íƒ
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--duration', '-d', help='í…ŒìŠ¤íŠ¸ ì§€ì†ì‹œê°„ (ì˜ˆ: 30s, 2m, 1h)')
    group.add_argument('--requests', '-r', type=int, help='ì´ ìš”ì²­ ìˆ˜')
    
    args = parser.parse_args()
    
    print(f"ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print(f"URL: {args.url}")
    print(f"ë™ì‹œì„±: {args.concurrency}")
    
    tester = LoadTester(args.url, args.concurrency)
    
    try:
        if args.duration:
            duration_seconds = parse_duration(args.duration)
            print(f"ì§€ì†ì‹œê°„: {duration_seconds}ì´ˆ")
            asyncio.run(tester.run_duration_test(duration_seconds))
        else:
            print(f"ì´ ìš”ì²­ ìˆ˜: {args.requests}")
            asyncio.run(tester.run_request_count_test(args.requests))
    
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        tester.print_results()


if __name__ == "__main__":
    main()