#!/usr/bin/env python3
"""
ìºì‹œ ì „ìš© ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - Lab 2ìš©

íŠ¹ì§•:
- ë™ì¼ ID ë°˜ë³µ ìš”ì²­ìœ¼ë¡œ ë†’ì€ ìºì‹œ íˆíŠ¸ìœ¨ ë‹¬ì„±
- ìºì‹œ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
- Before/After ë¹„êµìš©

ì‚¬ìš© ì˜ˆì‹œ:
python3 scripts/cache_load_test.py --url http://localhost:8000/api/v1/products --concurrency 50 --duration 30s --product-ids 1 42 100
"""
import argparse
import asyncio
import json
import logging
import random
import sys
import time
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import statistics

import aiohttp

# ìƒìœ„ ëª¨ë“ˆ importë¥¼ ìœ„í•œ path ì„¤ì •
sys.path.append(str(Path(__file__).parent))
from db_monitor import DatabaseMonitor


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    status_code: int
    response_time: float
    product_id: int
    error: Optional[str] = None
    timestamp: float = 0.0
    request_id: int = 0


class CacheLoadTester:
    """ìºì‹œ ì „ìš© ë¹„ë™ê¸° ë¶€í•˜ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str, concurrency: int, product_ids: List[int]):
        self.base_url = base_url.rstrip('/')
        self.concurrency = concurrency
        self.product_ids = product_ids
        self.results: List[TestResult] = []
        self.start_time = 0
        self.end_time = 0
        self.request_counter = 0
        self.results_dir = None
        self.logger = self._setup_logger()
        self.db_monitor = DatabaseMonitor()
        self.cache_metrics_start = None
        self.cache_metrics_end = None
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        # ê²°ê³¼ í´ë” ìƒì„±
        self.results_dir = Path('results') / f"cache_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        logger = logging.getLogger('cache_load_test')
        logger.setLevel(logging.INFO)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        log_file = self.results_dir / 'cache_load_test.log'
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¶”ê°€
        logger.handlers.clear()
        logger.addHandler(handler)
        
        return logger
    
    def _get_random_product_url(self) -> tuple[str, int]:
        """ëœë¤ ì œí’ˆ IDë¡œ URL ìƒì„±"""
        product_id = random.choice(self.product_ids)
        url = f"{self.base_url}/{product_id}"
        return url, product_id
    
    async def get_cache_metrics(self) -> Dict[str, Any]:
        """ì„œë²„ì—ì„œ ìºì‹œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # base_urlì—ì„œ í˜¸ìŠ¤íŠ¸ ì¶”ì¶œ
            import re
            match = re.match(r'(https?://[^/]+)', self.base_url)
            if match:
                base_host = match.group(1)
                metrics_url = f"{base_host}/metrics"
                
                timeout = aiohttp.ClientTimeout(total=5)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(metrics_url) as response:
                        if response.status == 200:
                            return await response.json()
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        
        return {}
    
    async def make_request(self, session: aiohttp.ClientSession) -> TestResult:
        """ë‹¨ì¼ HTTP ìš”ì²­ ì‹¤í–‰"""
        self.request_counter += 1
        request_id = self.request_counter
        timestamp = time.time()
        start = time.time()
        
        # ëœë¤ ì œí’ˆ IDë¡œ ìš”ì²­
        test_url, product_id = self._get_random_product_url()
        
        try:
            async with session.get(test_url) as response:
                await response.text()
                end = time.time()
                response_time = end - start
                response_time_ms = response_time * 1000
                
                # ìš”ì²­-ì‘ë‹µ ì‹œê°„ ë¡œê¹… (ms ë‹¨ìœ„)
                self.logger.info(f"Request {request_id}: {response.status} - {response_time_ms:.2f}ms - Product {product_id}")
                
                return TestResult(
                    status_code=response.status,
                    response_time=response_time,
                    product_id=product_id,
                    timestamp=timestamp,
                    request_id=request_id
                )
        except Exception as e:
            end = time.time()
            response_time = end - start
            response_time_ms = response_time * 1000
            
            # ì—ëŸ¬ ë¡œê¹…
            self.logger.error(f"Request {request_id}: ERROR - {response_time_ms:.2f}ms - Product {product_id} - {str(e)}")
            
            return TestResult(
                status_code=0,
                response_time=response_time,
                product_id=product_id,
                error=str(e),
                timestamp=timestamp,
                request_id=request_id
            )
    
    async def worker(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore, 
                    stop_event: asyncio.Event):
        """ì›Œì»¤ ì½”ë£¨í‹´ - ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´"""
        while not stop_event.is_set():
            async with semaphore:
                if stop_event.is_set():
                    break
                result = await self.make_request(session)
                self.results.append(result)
    
    async def run_duration_test(self, duration: int):
        """ì§€ì •ëœ ì‹œê°„ ë™ì•ˆ ìºì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        semaphore = asyncio.Semaphore(self.concurrency)
        stop_event = asyncio.Event()
        
        timeout = aiohttp.ClientTimeout(total=10)
        connector = aiohttp.TCPConnector(limit=self.concurrency * 2)
        
        # ì‹œì‘ ì „ ìºì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.cache_metrics_start = await self.get_cache_metrics()
        
        # DB ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.db_monitor.start_monitoring(interval=0.5)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            # ì›Œì»¤ íƒœìŠ¤í¬ë“¤ ìƒì„±
            workers = [
                asyncio.create_task(self.worker(session, semaphore, stop_event))
                for _ in range(self.concurrency)
            ]
            
            self.start_time = time.time()
            
            # ì§€ì •ëœ ì‹œê°„ ëŒ€ê¸°
            await asyncio.sleep(duration)
            
            # ëª¨ë“  ì›Œì»¤ ì¤‘ì§€
            stop_event.set()
            
            # ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
            await asyncio.gather(*workers, return_exceptions=True)
            
            self.end_time = time.time()
        
        # DB ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.db_monitor.stop_monitoring()
        
        # ì¢…ë£Œ í›„ ìºì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.cache_metrics_end = await self.get_cache_metrics()
    
    async def run_request_count_test(self, total_requests: int):
        """ì§€ì •ëœ ìš”ì²­ ìˆ˜ë§Œí¼ ìºì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        semaphore = asyncio.Semaphore(self.concurrency)
        
        timeout = aiohttp.ClientTimeout(total=10)
        connector = aiohttp.TCPConnector(limit=self.concurrency * 2)
        
        # ì‹œì‘ ì „ ìºì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.cache_metrics_start = await self.get_cache_metrics()
        
        # DB ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.db_monitor.start_monitoring(interval=0.5)
        
        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
            self.start_time = time.time()
            
            # íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            for _ in range(total_requests):
                task = asyncio.create_task(self.make_request_with_semaphore(session, semaphore))
                tasks.append(task)
            
            # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            self.end_time = time.time()
            
            # ê²°ê³¼ ì €ì¥
            for result in results:
                if isinstance(result, TestResult):
                    self.results.append(result)
        
        # DB ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.db_monitor.stop_monitoring()
        
        # ì¢…ë£Œ í›„ ìºì‹œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.cache_metrics_end = await self.get_cache_metrics()
    
    async def make_request_with_semaphore(self, session: aiohttp.ClientSession, 
                                        semaphore: asyncio.Semaphore) -> TestResult:
        """ì„¸ë§ˆí¬ì–´ì™€ í•¨ê»˜ ìš”ì²­ ì‹¤í–‰"""
        async with semaphore:
            return await self.make_request(session)
    
    def print_results(self):
        """ìºì‹œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        if not self.results:
            print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_time = self.end_time - self.start_time
        total_requests = len(self.results)
        
        # ìƒíƒœ ì½”ë“œë³„ í†µê³„
        status_counts = Counter(result.status_code for result in self.results)
        
        # ì œí’ˆ IDë³„ ìš”ì²­ ë¶„í¬
        product_id_counts = Counter(result.product_id for result in self.results)
        
        # ì„±ê³µí•œ ìš”ì²­ë§Œ í•„í„°ë§ (2xx, 3xx ìƒíƒœ ì½”ë“œ + ì—ëŸ¬ ì—†ìŒ)
        successful_requests = [r for r in self.results if r.error is None and 200 <= r.status_code < 400]
        response_times = [r.response_time for r in successful_requests]
        
        # ì—ëŸ¬ìœ¨ ê³„ì‚° (4xx, 5xx ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ)
        error_count = len([r for r in self.results if r.error is not None or r.status_code >= 400])
        error_rate = (error_count / total_requests) * 100 if total_requests > 0 else 0
        
        # QPS ê³„ì‚°
        qps = total_requests / total_time if total_time > 0 else 0
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        p50 = p95 = p99 = 0
        if response_times:
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)] * 1000  # ms ë³€í™˜
            p95 = sorted_times[int(len(sorted_times) * 0.95)] * 1000
            p99 = sorted_times[int(len(sorted_times) * 0.99)] * 1000
        
        print("=" * 60)
        print("ğŸš€ ìºì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 60)
        print(f"Base URL: {self.base_url}")
        print(f"ì œí’ˆ IDs: {self.product_ids}")
        print(f"ë™ì‹œì„± (Concurrency): {self.concurrency}")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"ì´ ìš”ì²­ ìˆ˜: {total_requests}")
        print(f"QPS: {qps:.2f}")
        
        print(f"\nğŸ“ˆ ìƒíƒœ ì½”ë“œë³„ í†µê³„:")
        for status_code, count in sorted(status_counts.items()):
            percentage = (count / total_requests) * 100
            status_emoji = "âœ…" if 200 <= status_code < 400 else "âŒ"
            print(f"  {status_emoji} {status_code}: {count}íšŒ ({percentage:.1f}%)")
        
        print(f"\nğŸ“Š ì œí’ˆ IDë³„ ìš”ì²­ ë¶„í¬:")
        for product_id, count in sorted(product_id_counts.items()):
            percentage = (count / total_requests) * 100
            print(f"  Product {product_id}: {count}íšŒ ({percentage:.1f}%)")
        
        print(f"\nâš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"  ì—ëŸ¬ìœ¨: {error_rate:.1f}% ({error_count}/{total_requests})")
        if response_times:
            print(f"  P50: {p50:.2f}ms, P95: {p95:.2f}ms, P99: {p99:.2f}ms")
        else:
            print(f"  ì„±ê³µ ìš”ì²­ì´ ì—†ì–´ ì‘ë‹µì‹œê°„ í†µê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìºì‹œ ë©”íŠ¸ë¦­ ì¶œë ¥
        if self.cache_metrics_start and self.cache_metrics_end:
            print(f"\nğŸ’¾ ìºì‹œ ë©”íŠ¸ë¦­:")
            start = self.cache_metrics_start
            end = self.cache_metrics_end
            
            hits_diff = end.get('hits', 0) - start.get('hits', 0)
            misses_diff = end.get('misses', 0) - start.get('misses', 0)
            sets_diff = end.get('sets', 0) - start.get('sets', 0)
            
            total_cache_requests = hits_diff + misses_diff
            hit_rate = (hits_diff / total_cache_requests * 100) if total_cache_requests > 0 else 0
            
            print(f"  ìºì‹œ íˆíŠ¸: {hits_diff}íšŒ")
            print(f"  ìºì‹œ ë¯¸ìŠ¤: {misses_diff}íšŒ") 
            print(f"  ìºì‹œ ì €ì¥: {sets_diff}íšŒ")
            print(f"  ìºì‹œ íˆíŠ¸ìœ¨: {hit_rate:.1f}%")
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
        
        # DB ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì¶œë ¥
        self.db_monitor.print_summary()
        
        print("=" * 60)
    
    def save_results(self):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.results:
            return
        
        total_time = self.end_time - self.start_time
        total_requests = len(self.results)
        
        # ì„±ê³µí•œ ìš”ì²­ë§Œ í•„í„°ë§
        successful_requests = [r for r in self.results if r.error is None and 200 <= r.status_code < 400]
        response_times = [r.response_time for r in successful_requests]
        
        # ì—ëŸ¬ìœ¨ ê³„ì‚°
        error_count = len([r for r in self.results if r.error is not None or r.status_code >= 400])
        error_rate = (error_count / total_requests) * 100 if total_requests > 0 else 0
        
        # QPS ê³„ì‚°
        qps = total_requests / total_time if total_time > 0 else 0
        
        # ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        p50 = p95 = p99 = 0
        if response_times:
            sorted_times = sorted(response_times)
            p50 = sorted_times[int(len(sorted_times) * 0.5)] * 1000
            p95 = sorted_times[int(len(sorted_times) * 0.95)] * 1000
            p99 = sorted_times[int(len(sorted_times) * 0.99)] * 1000
        
        # ìºì‹œ ë©”íŠ¸ë¦­ ê³„ì‚°
        cache_hits = cache_misses = cache_sets = cache_hit_rate = 0
        if self.cache_metrics_start and self.cache_metrics_end:
            start = self.cache_metrics_start
            end = self.cache_metrics_end
            cache_hits = end.get('hits', 0) - start.get('hits', 0)
            cache_misses = end.get('misses', 0) - start.get('misses', 0)
            cache_sets = end.get('sets', 0) - start.get('sets', 0)
            total_cache_requests = cache_hits + cache_misses
            cache_hit_rate = (cache_hits / total_cache_requests * 100) if total_cache_requests > 0 else 0
        
        baseline_data = {
            "test_type": "cache_load_test",
            "timestamp": datetime.now().isoformat(),
            "base_url": self.base_url,
            "product_ids": self.product_ids,
            "concurrency": self.concurrency,
            "duration_seconds": total_time,
            "total_requests": total_requests,
            "successful_requests": len(successful_requests),
            "qps": round(qps, 2),
            "error_rate_percent": round(error_rate, 2),
            "response_times_ms": {
                "p50": round(p50, 2),
                "p95": round(p95, 2),
                "p99": round(p99, 2),
                "avg": round(statistics.mean(response_times) * 1000, 2) if response_times else 0,
                "min": round(min(response_times) * 1000, 2) if response_times else 0,
                "max": round(max(response_times) * 1000, 2) if response_times else 0
            },
            "cache_metrics": {
                "hits": cache_hits,
                "misses": cache_misses,
                "sets": cache_sets,
                "hit_rate_percent": round(cache_hit_rate, 2)
            },
            "product_distribution": dict(Counter(r.product_id for r in self.results)),
            "status_codes": dict(Counter(r.status_code for r in self.results))
        }
        
        # baseline.json ì €ì¥
        baseline_file = self.results_dir / 'cache_baseline.json'
        with open(baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_results = {
            "test_summary": baseline_data,
            "raw_results": [
                {
                    "request_id": result.request_id,
                    "timestamp": result.timestamp,
                    "product_id": result.product_id,
                    "status_code": result.status_code,
                    "response_time_ms": round(result.response_time * 1000, 2),
                    "error": result.error,
                    "is_success": result.error is None and 200 <= result.status_code < 400
                }
                for result in self.results
            ]
        }
        
        detailed_file = self.results_dir / 'detailed_results.json'
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # DB ëª¨ë‹ˆí„°ë§ ê²°ê³¼ ì €ì¥
        db_stats_file = self.results_dir / 'db_monitoring.json'
        self.db_monitor.save_stats(db_stats_file)
        
        print(f"\nğŸ“ ê²°ê³¼ê°€ {self.results_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   - cache_baseline.json: ìºì‹œ ì„±ëŠ¥ ê¸°ì¤€ì„ ")
        print(f"   - detailed_results.json: ìƒì„¸ ê²°ê³¼")
        print(f"   - cache_load_test.log: ìš”ì²­/ì‘ë‹µ ë¡œê·¸")
        print(f"   - db_monitoring.json: DB ë¶€í•˜ ëª¨ë‹ˆí„°ë§")


def parse_duration(duration_str: str) -> int:
    """ì§€ì†ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: 30s, 2m, 1h)"""
    if duration_str.endswith('s'):
        return int(duration_str[:-1])
    elif duration_str.endswith('m'):
        return int(duration_str[:-1]) * 60
    elif duration_str.endswith('h'):
        return int(duration_str[:-1]) * 3600
    else:
        return int(duration_str)


def main():
    parser = argparse.ArgumentParser(description='ìºì‹œ ì „ìš© HTTP ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬')
    parser.add_argument('--url', required=True, help='ë² ì´ìŠ¤ URL (ì œí’ˆ ID ì—†ì´)')
    parser.add_argument('--concurrency', '-c', type=int, default=50, 
                       help='ë™ì‹œ ì—°ê²° ìˆ˜ (ê¸°ë³¸ê°’: 50)')
    parser.add_argument('--product-ids', nargs='+', type=int, default=[1, 42, 100],
                       help='í…ŒìŠ¤íŠ¸í•  ì œí’ˆ ID ëª©ë¡ (ê¸°ë³¸ê°’: 1 42 100)')
    
    # ì§€ì†ì‹œê°„ ë˜ëŠ” ìš”ì²­ ìˆ˜ ì¤‘ í•˜ë‚˜ ì„ íƒ
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--duration', '-d', help='í…ŒìŠ¤íŠ¸ ì§€ì†ì‹œê°„ (ì˜ˆ: 30s, 2m, 1h)')
    group.add_argument('--requests', '-r', type=int, help='ì´ ìš”ì²­ ìˆ˜')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ ìºì‹œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print(f"Base URL: {args.url}")
    print(f"ì œí’ˆ IDs: {args.product_ids}")
    print(f"ë™ì‹œì„±: {args.concurrency}")
    
    tester = CacheLoadTester(args.url, args.concurrency, args.product_ids)
    
    try:
        if args.duration:
            duration_seconds = parse_duration(args.duration)
            print(f"ì§€ì†ì‹œê°„: {duration_seconds}ì´ˆ")
            asyncio.run(tester.run_duration_test(duration_seconds))
        else:
            print(f"ì´ ìš”ì²­ ìˆ˜: {args.requests}")
            asyncio.run(tester.run_request_count_test(args.requests))
    
    except KeyboardInterrupt:
        print("\ní…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        tester.print_results()


if __name__ == "__main__":
    main()